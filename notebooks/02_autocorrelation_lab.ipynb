{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü¶∑ Autocorrelation Lab ‚Äî ISQ Trajectory\n",
        "\n",
        "## üìã Goal\n",
        "\n",
        "Detect serial correlation in repeated ISQ measurements and compare statistical remedies to produce valid inference.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Clinical Context\n",
        "\n",
        "### The ISQ Healing Timeline\n",
        "\n",
        "After dental implant placement, primary stability gradually transitions to **secondary (biological) stability** through osseointegration. We track this using **ISQ (Implant Stability Quotient)** measured weekly:\n",
        "\n",
        "- **Weeks 0-2**: High primary stability (mechanical anchoring)\n",
        "- **Weeks 2-4**: **\"Stability dip\"** as bone remodeling begins\n",
        "- **Weeks 4-8**: Recovery as new bone forms (osseointegration)\n",
        "\n",
        "### The Statistical Problem: Autocorrelation\n",
        "\n",
        "When we measure the same implant repeatedly over time, consecutive measurements are **not independent**:\n",
        "\n",
        "- If ISQ is high at week 2, it's likely high at week 3\n",
        "- A patient's biology creates **persistent patterns** across measurements\n",
        "- Standard OLS regression **assumes independence** ‚Üí invalid standard errors\n",
        "\n",
        "**Autocorrelation** (serial correlation) violates the independence assumption, causing:\n",
        "- ‚ùå **Underestimated standard errors** (false confidence)\n",
        "- ‚ùå **Inflated t-statistics** (spurious significance)\n",
        "- ‚ùå **Invalid hypothesis tests** (Type I errors)\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "1. **Visualize ISQ trajectories** to see dip-and-recovery patterns\n",
        "2. **Diagnose autocorrelation** using ACF plots, Durbin-Watson, and Ljung-Box tests\n",
        "3. **Apply remedies**: lagged predictors, GLS with AR(1) errors, HAC robust standard errors\n",
        "4. **Choose the right approach** for clinical time-series data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 0Ô∏è‚É£ Setup & Data Reshaping\n",
        "\n",
        "### üìä Wide vs. Long Format\n",
        "\n",
        "Our dataset is currently in **wide format**:\n",
        "```\n",
        "implant_id | torque_ncm | isq_w0 | isq_w1 | isq_w2 | ... | isq_w8\n",
        "1          | 35         | 72     | 71     | 68     | ... | 74\n",
        "```\n",
        "\n",
        "For time-series analysis, we need **long format**:\n",
        "```\n",
        "implant_id | week | isq | torque_ncm\n",
        "1          | 0    | 72  | 35\n",
        "1          | 1    | 71  | 35\n",
        "1          | 2    | 68  | 35\n",
        "...\n",
        "```\n",
        "\n",
        "### üîß Why Long Format?\n",
        "\n",
        "1. **Each row is one observation** at one time point\n",
        "2. Easier to model `isq ~ week + week¬≤`\n",
        "3. Natural structure for time-series models (GLS, ARIMA)\n",
        "4. Allows grouping by `implant_id` for lagged variables\n",
        "\n",
        "### üì¶ Tools We'll Use\n",
        "\n",
        "- **Pandas**: `pd.melt()` for reshaping\n",
        "- **Matplotlib**: trajectory plots\n",
        "- **Statsmodels**: ACF, Durbin-Watson, Ljung-Box, GLS, HAC standard errors\n",
        "\n",
        "Let's start:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: load the same CSV as df\n",
        "# Hint: \n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import statsmodels.api as sm\n",
        "# import statsmodels.stats.api as sms\n",
        "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "# from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "#\n",
        "# df = pd.read_csv('../data/raw/implants_stability_300.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: reshape ISQ columns (isq_w0...isq_w8) from wide to long -> df_long with columns: implant_id, week, isq\n",
        "# hint: pd.melt with id_vars=[...], value_vars=[...]\n",
        "#\n",
        "# Hint:\n",
        "# # Identify ISQ columns\n",
        "# isq_cols = [col for col in df.columns if col.startswith('isq_w')]\n",
        "# \n",
        "# # Melt to long format\n",
        "# df_long = pd.melt(\n",
        "#     df,\n",
        "#     id_vars=['implant_id', 'torque_ncm', 'bic_percent', 'cortical_thickness_mm', \n",
        "#              'diameter_mm', 'arch', 'site_type', 'bone_loss_12m_mm'],\n",
        "#     value_vars=isq_cols,\n",
        "#     var_name='week_label',\n",
        "#     value_name='isq'\n",
        "# )\n",
        "#\n",
        "# # Extract numeric week from 'isq_w0' -> 0\n",
        "# df_long['week'] = df_long['week_label'].str.extract('(\\d+)').astype(int)\n",
        "# df_long = df_long.drop('week_label', axis=1).sort_values(['implant_id', 'week'])\n",
        "#\n",
        "# print(f\"Wide format: {df.shape}\")\n",
        "# print(f\"Long format: {df_long.shape}\")\n",
        "# print(df_long.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Data Check\n",
        "\n",
        "After reshaping, verify:\n",
        "- Each implant has **7-8 rows** (weeks 0, 1, 2, 3, 4, 6, 8)\n",
        "- No missing ISQ values (or note which weeks are missing)\n",
        "- Baseline variables (torque, cortical thickness) are **repeated** for each time point\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Visual Check: ISQ Trajectories\n",
        "\n",
        "### üìà The Classic \"Dip and Recovery\" Pattern\n",
        "\n",
        "Before running any statistics, **visualize the data**. In implant dentistry, we expect:\n",
        "\n",
        "1. **Week 0**: High ISQ (primary stability from mechanical fit)\n",
        "2. **Weeks 2-3**: **Dip** (bone remodeling, temporary instability)\n",
        "3. **Weeks 6-8**: **Recovery** (osseointegration, secondary stability)\n",
        "\n",
        "### üß† Clinical Insight\n",
        "\n",
        "Not all implants follow this pattern:\n",
        "- **Type 1 bone (dense cortical)**: Minimal dip, high stability throughout\n",
        "- **Type 4 bone (soft trabecular)**: Deep dip, slow recovery\n",
        "- **Immediate loading**: May show different trajectory\n",
        "\n",
        "### üîç What to Look For\n",
        "\n",
        "- **Individual variability**: Do all implants dip at the same time?\n",
        "- **Baseline effects**: Do high-torque implants recover faster?\n",
        "- **Autocorrelation clue**: Smooth curves ‚Üí consecutive measurements are correlated\n",
        "\n",
        "Let's plot a few trajectories:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: pick a few implant_ids and plot ISQ vs week (plt.plot) to see dip/recovery\n",
        "#\n",
        "# Hint:\n",
        "# # Select 10 random implants\n",
        "# sample_ids = df_long['implant_id'].unique()[:10]\n",
        "# df_sample = df_long[df_long['implant_id'].isin(sample_ids)]\n",
        "#\n",
        "# # Plot trajectories\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# for implant_id in sample_ids:\n",
        "#     implant_data = df_sample[df_sample['implant_id'] == implant_id]\n",
        "#     plt.plot(implant_data['week'], implant_data['isq'], marker='o', alpha=0.6, label=f'ID {implant_id}')\n",
        "#\n",
        "# plt.axhline(y=65, color='red', linestyle='--', alpha=0.5, label='Clinical threshold (65)')\n",
        "# plt.xlabel('Week')\n",
        "# plt.ylabel('ISQ')\n",
        "# plt.title('ISQ Trajectories: Individual Implants Over Time')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "# plt.grid(True, alpha=0.3)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "#\n",
        "# # Optional: Aggregate trajectory (mean ¬± SE)\n",
        "# agg = df_long.groupby('week')['isq'].agg(['mean', 'sem']).reset_index()\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.errorbar(agg['week'], agg['mean'], yerr=agg['sem'], marker='o', capsize=5, linewidth=2)\n",
        "# plt.xlabel('Week')\n",
        "# plt.ylabel('Mean ISQ ¬± SE')\n",
        "# plt.title('Average ISQ Trajectory (N=300 implants)')\n",
        "# plt.grid(True, alpha=0.3)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Reflection Questions\n",
        "\n",
        "After visualizing trajectories:\n",
        "1. Do you see the classic dip-and-recovery pattern?\n",
        "2. At what week does the average ISQ reach its lowest point?\n",
        "3. Do individual trajectories look \"smooth\"? (If yes, that's a sign of autocorrelation)\n",
        "4. Are there outlier implants with unusual patterns?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Simple Time-Series Regression\n",
        "\n",
        "### üìä Baseline Model\n",
        "\n",
        "We'll model the ISQ trajectory using a **quadratic time trend**:\n",
        "\n",
        "$$\\text{ISQ}_{it} = \\beta_0 + \\beta_1 \\cdot \\text{week}_t + \\beta_2 \\cdot \\text{week}_t^2 + \\beta_3 \\cdot \\text{torque}_i + \\epsilon_{it}$$\n",
        "\n",
        "Where:\n",
        "- $i$ = implant index\n",
        "- $t$ = time (week)\n",
        "- **Linear term** ($\\beta_1$): Overall trend (negative = decline)\n",
        "- **Quadratic term** ($\\beta_2$): Curvature (positive = U-shape recovery)\n",
        "\n",
        "### üîß Why This Model?\n",
        "\n",
        "- **Quadratic** captures the dip-and-recovery shape\n",
        "- **Torque** controls for baseline stability differences\n",
        "- **Additive**: Can include other predictors (bone density, arch, etc.)\n",
        "\n",
        "### ‚ö†Ô∏è The Problem\n",
        "\n",
        "Standard OLS assumes **independent errors**: $\\epsilon_{i1}, \\epsilon_{i2}, \\epsilon_{i3}$ are uncorrelated.\n",
        "\n",
        "But in reality: if $\\epsilon_{i,\\text{week2}}$ is positive (ISQ higher than expected), then $\\epsilon_{i,\\text{week3}}$ is likely also positive.\n",
        "\n",
        "**This is autocorrelation**, and it invalidates our standard errors.\n",
        "\n",
        "Let's fit the model and save residuals for diagnosis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: merge baseline predictors (e.g., torque_ncm, bone_density) into df_long (by implant_id)\n",
        "# (Already done during melt‚Äîbaseline vars are repeated for each time point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: fit OLS: isq ~ week + week^2 + torque_ncm (+ other controls)\n",
        "#\n",
        "# Hint:\n",
        "# # Create polynomial term\n",
        "# df_long['week_squared'] = df_long['week'] ** 2\n",
        "#\n",
        "# # Define predictors\n",
        "# X = df_long[['week', 'week_squared', 'torque_ncm', 'cortical_thickness_mm']]\n",
        "# y = df_long['isq']\n",
        "#\n",
        "# # Drop missing values\n",
        "# mask = X.notna().all(axis=1) & y.notna()\n",
        "# X_clean = sm.add_constant(X[mask])\n",
        "# y_clean = y[mask]\n",
        "#\n",
        "# # Fit OLS\n",
        "# ols_model = sm.OLS(y_clean, X_clean).fit()\n",
        "# print(ols_model.summary())\n",
        "#\n",
        "# # Interpretation:\n",
        "# # - week coefficient: linear trend (negative = decline early on)\n",
        "# # - week_squared coefficient: curvature (positive = U-shape)\n",
        "# # - torque_ncm: effect of baseline stability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: store residuals\n",
        "#\n",
        "# Hint:\n",
        "# residuals = ols_model.resid\n",
        "# df_long.loc[mask, 'residuals'] = residuals\n",
        "#\n",
        "# # Quick check: residuals should sum to ~0\n",
        "# print(f\"Mean residual: {residuals.mean():.6f}\")\n",
        "# print(f\"Std residual: {residuals.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Model Interpretation\n",
        "\n",
        "After fitting, check:\n",
        "- **Week coefficient**: Is it negative? (Indicates initial decline)\n",
        "- **Week¬≤ coefficient**: Is it positive? (Indicates recovery/U-shape)\n",
        "- **R¬≤**: How much variance does the time trend explain?\n",
        "\n",
        "**But don't trust the p-values yet**‚Äîwe haven't tested for autocorrelation!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Autocorrelation Diagnostics\n",
        "\n",
        "### üîç Three Tools to Detect Serial Correlation\n",
        "\n",
        "Now we'll apply **three complementary diagnostics** to check if residuals are autocorrelated:\n",
        "\n",
        "1. **ACF Plot (Autocorrelation Function)**: Visual check for correlation at different lags\n",
        "2. **Durbin-Watson Statistic**: Tests first-order autocorrelation (lag-1)\n",
        "3. **Ljung-Box Test**: Formal test for autocorrelation up to lag $k$\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Tool 1: ACF Plot\n",
        "\n",
        "#### What is ACF?\n",
        "\n",
        "The **autocorrelation function** measures correlation between $\\epsilon_t$ and $\\epsilon_{t-k}$ at lag $k$:\n",
        "\n",
        "$$\\rho_k = \\text{Corr}(\\epsilon_t, \\epsilon_{t-k})$$\n",
        "\n",
        "- **Lag 1**: Correlation between consecutive weeks (week 0 & week 1)\n",
        "- **Lag 2**: Correlation two weeks apart (week 0 & week 2)\n",
        "\n",
        "#### üéØ Interpretation\n",
        "\n",
        "- **Bars inside blue band**: No significant autocorrelation (‚úÖ good!)\n",
        "- **Bars outside blue band**: Significant autocorrelation (‚ö†Ô∏è problem!)\n",
        "- **Slow decay**: Strong persistent correlation (common in time series)\n",
        "\n",
        "#### ü¶∑ Clinical Meaning\n",
        "\n",
        "If lag-1 ACF is high: \"An implant with high ISQ at week 2 tends to have high ISQ at week 3.\"\n",
        "\n",
        "This violates independence ‚Üí standard errors are wrong.\n",
        "\n",
        "Let's visualize:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot ACF of residuals (statsmodels.graphics.tsaplots.plot_acf)\n",
        "#\n",
        "# Hint:\n",
        "# from statsmodels.graphics.tsaplots import plot_acf\n",
        "#\n",
        "# # Plot ACF with 95% confidence bands\n",
        "# fig, ax = plt.subplots(figsize=(10, 5))\n",
        "# plot_acf(residuals, lags=6, ax=ax, alpha=0.05)\n",
        "# ax.set_xlabel('Lag (weeks)')\n",
        "# ax.set_ylabel('Autocorrelation')\n",
        "# ax.set_title('ACF of OLS Residuals')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "#\n",
        "# # Interpretation:\n",
        "# # - If lag-1 bar exceeds blue band ‚Üí positive autocorrelation\n",
        "# # - If multiple lags significant ‚Üí persistent serial correlation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üî¢ Tool 2: Durbin-Watson Statistic\n",
        "\n",
        "#### What is Durbin-Watson?\n",
        "\n",
        "The **Durbin-Watson (DW)** statistic tests for **first-order** (lag-1) autocorrelation:\n",
        "\n",
        "$$DW = \\frac{\\sum_{t=2}^n (\\epsilon_t - \\epsilon_{t-1})^2}{\\sum_{t=1}^n \\epsilon_t^2}$$\n",
        "\n",
        "#### üéØ Interpretation\n",
        "\n",
        "| DW Value | Interpretation |\n",
        "|----------|----------------|\n",
        "| **0** | Perfect positive autocorrelation |\n",
        "| **2** | No autocorrelation (‚úÖ ideal) |\n",
        "| **4** | Perfect negative autocorrelation |\n",
        "| **1.5 - 2.5** | Acceptable range |\n",
        "| **< 1 or > 3** | Serious autocorrelation |\n",
        "\n",
        "#### üß† Rule of Thumb\n",
        "\n",
        "- **DW ‚âà 2**: Independent errors (OLS assumptions met)\n",
        "- **DW < 2**: Positive autocorrelation (consecutive errors have same sign)\n",
        "- **DW > 2**: Negative autocorrelation (rare in practice)\n",
        "\n",
        "#### ü¶∑ Clinical Context\n",
        "\n",
        "In ISQ data, we almost always see **positive autocorrelation** (DW < 2) because biological processes are smooth and persistent.\n",
        "\n",
        "Let's compute:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: compute Durbin‚ÄìWatson (sms.durbin_watson) and Ljung‚ÄìBox (statsmodels.stats.diagnostic.acorr_ljungbox)\n",
        "#\n",
        "# Hint:\n",
        "# # Durbin-Watson\n",
        "# dw = sms.durbin_watson(residuals)\n",
        "# print(f\"Durbin-Watson Statistic: {dw:.3f}\")\n",
        "# if dw < 1.5:\n",
        "#     print(\"‚ö†Ô∏è  Strong positive autocorrelation detected\")\n",
        "# elif dw > 2.5:\n",
        "#     print(\"‚ö†Ô∏è  Negative autocorrelation detected (rare)\")\n",
        "# else:\n",
        "#     print(\"‚úÖ No serious autocorrelation (but check Ljung-Box)\")\n",
        "#\n",
        "# # Ljung-Box Test\n",
        "# lb_test = acorr_ljungbox(residuals, lags=[1, 2, 3], return_df=True)\n",
        "# print(\"\\nLjung-Box Test:\")\n",
        "# print(lb_test)\n",
        "#\n",
        "# # Interpretation:\n",
        "# # - lb_pvalue < 0.05 ‚Üí reject H0 (no autocorrelation) ‚Üí autocorrelation present\n",
        "# # - Check multiple lags to see persistence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üìä Tool 3: Ljung-Box Test\n",
        "\n",
        "#### What is Ljung-Box?\n",
        "\n",
        "The **Ljung-Box test** is a formal hypothesis test for autocorrelation:\n",
        "\n",
        "- **H‚ÇÄ**: No autocorrelation up to lag $k$\n",
        "- **H‚ÇÅ**: Autocorrelation exists at one or more lags\n",
        "\n",
        "Test statistic:\n",
        "$$Q = n(n+2) \\sum_{k=1}^h \\frac{\\hat{\\rho}_k^2}{n-k}$$\n",
        "\n",
        "Where $\\hat{\\rho}_k$ is the sample autocorrelation at lag $k$.\n",
        "\n",
        "#### üéØ Interpretation\n",
        "\n",
        "- **p-value < 0.05**: Reject H‚ÇÄ ‚Üí autocorrelation detected\n",
        "- **p-value ‚â• 0.05**: Fail to reject H‚ÇÄ ‚Üí no evidence of autocorrelation\n",
        "\n",
        "#### ‚úÖ Diagnostic Summary\n",
        "\n",
        "After running all three diagnostics, ask:\n",
        "1. Does the ACF plot show bars outside the confidence band?\n",
        "2. Is Durbin-Watson significantly different from 2?\n",
        "3. Do Ljung-Box p-values indicate rejection at multiple lags?\n",
        "\n",
        "If **yes to 2+**, autocorrelation is present ‚Üí we need remedies.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Remedies and Comparison\n",
        "\n",
        "### üõ† Three Approaches to Fix Autocorrelation\n",
        "\n",
        "Now that we've **diagnosed** the problem, let's **fix** it. We'll try three remedies:\n",
        "\n",
        "| Remedy | Strategy | Pros | Cons |\n",
        "|--------|----------|------|------|\n",
        "| **Lagged ISQ** | Add $\\text{ISQ}_{t-1}$ as predictor | Simple, interpretable | Changes research question |\n",
        "| **GLS with AR(1)** | Model errors as $\\epsilon_t = \\rho \\epsilon_{t-1} + \\nu_t$ | Correct inference, elegant | Requires stationarity |\n",
        "| **HAC Standard Errors** | Adjust SEs for correlation | Doesn't change model | Only fixes SEs, not estimates |\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Remedy 1: Add Lagged ISQ\n",
        "\n",
        "#### üí° Idea\n",
        "\n",
        "If consecutive measurements are correlated, **include past ISQ as a predictor**:\n",
        "\n",
        "$$\\text{ISQ}_t = \\beta_0 + \\beta_1 \\cdot \\text{week} + \\beta_2 \\cdot \\text{week}^2 + \\beta_3 \\cdot \\text{ISQ}_{t-1} + \\epsilon_t$$\n",
        "\n",
        "#### ‚úÖ Why This Works\n",
        "\n",
        "- **Lagged ISQ captures persistence**: High ISQ at $t-1$ predicts high ISQ at $t$\n",
        "- Residuals become **more independent** (lower ACF)\n",
        "- Standard errors are now trustworthy\n",
        "\n",
        "#### ‚ùå Trade-Off\n",
        "\n",
        "- **Changes interpretation**: Now modeling \"change in ISQ\" not \"absolute ISQ\"\n",
        "- **Loses first observation** (no lag for week 0)\n",
        "- **Different research question**: \"Does ISQ predict future ISQ?\" vs. \"How does ISQ evolve?\"\n",
        "\n",
        "#### ü¶∑ Clinical Use\n",
        "\n",
        "Best when: \"We want to predict next week's ISQ given this week's measurement.\"\n",
        "\n",
        "Let's try it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: add lagged isq (grouped by implant_id) -> isq_lag1; refit model and compare residual ACF\n",
        "#\n",
        "# Hint:\n",
        "# # Create lag-1 ISQ (grouped by implant to avoid cross-implant leakage)\n",
        "# df_long = df_long.sort_values(['implant_id', 'week'])\n",
        "# df_long['isq_lag1'] = df_long.groupby('implant_id')['isq'].shift(1)\n",
        "#\n",
        "# # Refit model with lagged term\n",
        "# X_lag = df_long[['week', 'week_squared', 'torque_ncm', 'cortical_thickness_mm', 'isq_lag1']]\n",
        "# y_lag = df_long['isq']\n",
        "#\n",
        "# mask_lag = X_lag.notna().all(axis=1) & y_lag.notna()\n",
        "# X_lag_clean = sm.add_constant(X_lag[mask_lag])\n",
        "# y_lag_clean = y_lag[mask_lag]\n",
        "#\n",
        "# ols_lag_model = sm.OLS(y_lag_clean, X_lag_clean).fit()\n",
        "# print(ols_lag_model.summary())\n",
        "#\n",
        "# # Check new residuals\n",
        "# residuals_lag = ols_lag_model.resid\n",
        "# dw_lag = sms.durbin_watson(residuals_lag)\n",
        "# print(f\"\\nDurbin-Watson (with lag): {dw_lag:.3f} (was {dw:.3f})\")\n",
        "#\n",
        "# # Plot new ACF\n",
        "# fig, ax = plt.subplots(figsize=(10, 5))\n",
        "# plot_acf(residuals_lag, lags=6, ax=ax, alpha=0.05)\n",
        "# ax.set_title('ACF of Residuals: Model with Lagged ISQ')\n",
        "# plt.show()\n",
        "#\n",
        "# # Did it help?\n",
        "# print(f\"Improvement: DW moved from {dw:.3f} ‚Üí {dw_lag:.3f} (closer to 2 = better)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üìä Remedy 2: GLS with AR(1) Errors\n",
        "\n",
        "#### üí° Idea\n",
        "\n",
        "Instead of changing the model, **model the error structure** directly.\n",
        "\n",
        "**Generalized Least Squares (GLS)** assumes errors follow an autoregressive process:\n",
        "\n",
        "$$\\epsilon_t = \\rho \\cdot \\epsilon_{t-1} + \\nu_t$$\n",
        "\n",
        "Where:\n",
        "- $\\rho$ = autocorrelation coefficient (estimated from data)\n",
        "- $\\nu_t$ = white noise (independent errors)\n",
        "\n",
        "#### üßÆ How It Works\n",
        "\n",
        "1. **Estimate $\\rho$** from OLS residuals\n",
        "2. **Transform the data** to remove autocorrelation: $y_t^* = y_t - \\rho y_{t-1}$\n",
        "3. **Refit using transformed data** ‚Üí correct standard errors\n",
        "\n",
        "#### ‚úÖ Advantages\n",
        "\n",
        "- **Doesn't change predictors**: Research question stays the same\n",
        "- **Theoretically elegant**: Explicitly models persistence\n",
        "- **Efficient estimates**: Better than OLS when errors are AR(1)\n",
        "\n",
        "#### ‚ùå Disadvantages\n",
        "\n",
        "- **Assumes AR(1)**: What if errors follow AR(2) or more complex pattern?\n",
        "- **Requires stationarity**: May not work if trend is strong\n",
        "- **Loses observations**: First observation dropped\n",
        "\n",
        "#### ü¶∑ Clinical Use\n",
        "\n",
        "Best when: \"ISQ trajectories are smooth, and we want to correct inference without changing the model.\"\n",
        "\n",
        "Let's fit GLS:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: fit GLS with AR(1) errors and compare summary to OLS\n",
        "# hint: statsmodels.regression.linear_model.GLSAR or sm.tsa.statespace.SARIMAX\n",
        "#\n",
        "# Hint:\n",
        "# from statsmodels.regression.linear_model import GLSAR\n",
        "#\n",
        "# # Prepare data (same as OLS)\n",
        "# X_gls = df_long[['week', 'week_squared', 'torque_ncm', 'cortical_thickness_mm']]\n",
        "# y_gls = df_long['isq']\n",
        "# mask_gls = X_gls.notna().all(axis=1) & y_gls.notna()\n",
        "# X_gls_clean = sm.add_constant(X_gls[mask_gls])\n",
        "# y_gls_clean = y_gls[mask_gls]\n",
        "#\n",
        "# # Fit GLSAR (iteratively estimates rho)\n",
        "# gls_model = GLSAR(y_gls_clean, X_gls_clean, rho=1).fit()\n",
        "# print(gls_model.summary())\n",
        "#\n",
        "# # Compare standard errors\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"Coefficient Comparison: OLS vs. GLS\")\n",
        "# print(\"=\"*60)\n",
        "# comparison = pd.DataFrame({\n",
        "#     'OLS_coef': ols_model.params,\n",
        "#     'OLS_se': ols_model.bse,\n",
        "#     'GLS_coef': gls_model.params,\n",
        "#     'GLS_se': gls_model.bse\n",
        "# })\n",
        "# print(comparison)\n",
        "#\n",
        "# # Key insight: GLS standard errors are typically LARGER (more honest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç OLS vs. GLS: What Changed?\n",
        "\n",
        "After fitting GLS, compare:\n",
        "1. **Coefficients**: Did they change? (Usually only slightly)\n",
        "2. **Standard errors**: Are GLS SEs larger? (Usually yes‚Äîmore conservative)\n",
        "3. **P-values**: Did significance change? (Critical for hypothesis testing)\n",
        "4. **Estimated œÅ**: What's the autocorrelation coefficient? (Should be 0-1)\n",
        "\n",
        "**Key takeaway**: OLS gives **overconfident** p-values when autocorrelation is present. GLS corrects this.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ°Ô∏è Remedy 3: HAC (Heteroskedasticity and Autocorrelation Consistent) Standard Errors\n",
        "\n",
        "#### üí° Idea\n",
        "\n",
        "Keep the OLS model **exactly as is**, but compute **robust standard errors** that account for autocorrelation.\n",
        "\n",
        "Also called **Newey-West standard errors**.\n",
        "\n",
        "#### üßÆ How It Works\n",
        "\n",
        "Instead of assuming $\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}$, use a **sandwich estimator**:\n",
        "\n",
        "$$\\text{Var}(\\hat{\\beta}) = (X'X)^{-1} \\Omega (X'X)^{-1}$$\n",
        "\n",
        "Where $\\Omega$ accounts for serial correlation up to lag $L$.\n",
        "\n",
        "#### ‚úÖ Advantages\n",
        "\n",
        "- **Simple**: Just adjust standard errors, don't refit\n",
        "- **Flexible**: Works for any autocorrelation pattern\n",
        "- **Doesn't change estimates**: Coefficients stay the same\n",
        "\n",
        "#### ‚ùå Disadvantages\n",
        "\n",
        "- **Only fixes SEs**: Point estimates are still inefficient (not optimal)\n",
        "- **Requires choosing lag length** ($L$): How many lags to correct for?\n",
        "- **Large-sample approximation**: May not work well with small N\n",
        "\n",
        "#### ü¶∑ Clinical Use\n",
        "\n",
        "Best when: \"We want to report OLS estimates but acknowledge autocorrelation in confidence intervals.\"\n",
        "\n",
        "Let's compute HAC standard errors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: compute HAC/Newey‚ÄìWest SEs on the OLS fit and compare standard errors\n",
        "# hint: results.get_robustcov_results(cov_type='HAC', maxlags=1 or 2)\n",
        "#\n",
        "# Hint:\n",
        "# # Get HAC-robust results from original OLS model\n",
        "# ols_hac = ols_model.get_robustcov_results(cov_type='HAC', maxlags=2)\n",
        "# print(ols_hac.summary())\n",
        "#\n",
        "# # Compare all three approaches\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"STANDARD ERROR COMPARISON: OLS vs. GLS vs. HAC\")\n",
        "# print(\"=\"*80)\n",
        "# se_comparison = pd.DataFrame({\n",
        "#     'Variable': ols_model.params.index,\n",
        "#     'OLS_se': ols_model.bse.values,\n",
        "#     'GLS_se': gls_model.bse.values,\n",
        "#     'HAC_se': ols_hac.bse.values\n",
        "# })\n",
        "# print(se_comparison)\n",
        "#\n",
        "# # Calculate percentage increase\n",
        "# se_comparison['OLS_to_HAC_%'] = ((se_comparison['HAC_se'] - se_comparison['OLS_se']) / se_comparison['OLS_se'] * 100)\n",
        "# print(\"\\nPercentage increase in SEs (OLS ‚Üí HAC):\")\n",
        "# print(se_comparison[['Variable', 'OLS_to_HAC_%']])\n",
        "#\n",
        "# # Key insight: HAC SEs are larger ‚Üí wider CIs ‚Üí more honest inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Remedy Comparison Summary\n",
        "\n",
        "Create a comparison table:\n",
        "\n",
        "| Metric | OLS Baseline | Lagged ISQ | GLS AR(1) | HAC SEs |\n",
        "|--------|--------------|------------|-----------|---------|\n",
        "| **Coefficients** | Original | Different | Similar | Same as OLS |\n",
        "| **Standard Errors** | Underestimated | Corrected | Corrected | Corrected |\n",
        "| **Durbin-Watson** | ~1.2 | ~1.9 | N/A | ~1.2 |\n",
        "| **Model Interpretation** | Time trend | Autoregressive | Time trend | Time trend |\n",
        "| **Research Question** | \"How does ISQ evolve?\" | \"Does past ISQ predict future?\" | \"How does ISQ evolve?\" | \"How does ISQ evolve?\" |\n",
        "\n",
        "### ü§î Which Remedy to Use?\n",
        "\n",
        "**Use Lagged ISQ when:**\n",
        "- You care about prediction (\"What will next week's ISQ be?\")\n",
        "- Autocorrelation is very strong (DW < 1)\n",
        "- You're comfortable with a dynamic model\n",
        "\n",
        "**Use GLS when:**\n",
        "- You want to preserve the original research question\n",
        "- Errors are well-approximated by AR(1)\n",
        "- You have sufficient data (N > 100)\n",
        "\n",
        "**Use HAC SEs when:**\n",
        "- You're reporting OLS estimates to match literature\n",
        "- You're uncertain about the error structure\n",
        "- You want a \"quick fix\" without refitting\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Conclusions & Clinical Recommendations\n",
        "\n",
        "### üéØ What Changed Across Remedies?\n",
        "\n",
        "After trying all three approaches, reflect on:\n",
        "\n",
        "1. **Standard errors**: How much did they increase? (10%? 30%? More?)\n",
        "2. **Significance**: Did any variables lose significance after correction?\n",
        "3. **Coefficient interpretation**: Did the story change?\n",
        "4. **Durbin-Watson**: Did adding lags reduce autocorrelation?\n",
        "\n",
        "### üìä Statistical Defensibility\n",
        "\n",
        "| Approach | Statistically Defensible? | When to Use |\n",
        "|----------|---------------------------|-------------|\n",
        "| **Naive OLS** | ‚ùå No | Never with time-series data |\n",
        "| **Lagged ISQ** | ‚úÖ Yes | When prediction is the goal |\n",
        "| **GLS AR(1)** | ‚úÖ Yes | When errors are AR(1) |\n",
        "| **HAC SEs** | ‚úÖ Yes | When you want conservative inference |\n",
        "\n",
        "### ü¶∑ Clinical Defensibility\n",
        "\n",
        "From a clinical perspective:\n",
        "\n",
        "**Lagged ISQ**:\n",
        "- ‚úÖ Makes sense: \"Today's stability predicts tomorrow's\"\n",
        "- ‚ùå Doesn't answer: \"What's the overall healing trajectory?\"\n",
        "\n",
        "**GLS AR(1)**:\n",
        "- ‚úÖ Preserves interpretation: \"Week effects on ISQ\"\n",
        "- ‚úÖ Acknowledges biology: Smooth, persistent healing\n",
        "\n",
        "**HAC SEs**:\n",
        "- ‚úÖ Conservative: Wider CIs = honest uncertainty\n",
        "- ‚úÖ Simple to explain: \"Same model, more cautious inference\"\n",
        "\n",
        "### üî¨ Reporting Recommendations\n",
        "\n",
        "When publishing, report:\n",
        "1. **Baseline OLS** (for comparison to literature)\n",
        "2. **Diagnostic results** (DW, ACF plot, Ljung-Box)\n",
        "3. **Preferred remedy** with justification (GLS or HAC)\n",
        "4. **Sensitivity analysis**: Show that conclusions hold across methods\n",
        "\n",
        "### ‚úçÔ∏è Sample Text for Methods Section\n",
        "\n",
        "> \"To account for serial correlation in repeated ISQ measurements, we applied three approaches: (1) inclusion of lagged ISQ as a predictor, (2) generalized least squares with AR(1) errors, and (3) Newey-West heteroskedasticity- and autocorrelation-consistent standard errors. Durbin-Watson statistics indicated positive first-order autocorrelation (DW = X.XX). Primary results are reported using GLS, with HAC-robust standard errors provided for sensitivity.\"\n",
        "\n",
        "---\n",
        "\n",
        "Now write your summary:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: write markdown/print summary\n",
        "#\n",
        "# Example structure:\n",
        "# print(\"\"\"\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üìä AUTOCORRELATION LAB SUMMARY\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "#\n",
        "# 1. DATA STRUCTURE\n",
        "#    - Wide format: N=300 implants √ó 8 time points\n",
        "#    - Long format: N=2,400 observations (300 √ó 8)\n",
        "#    - Outcome: ISQ (0-100 scale)\n",
        "#\n",
        "# 2. VISUALIZATION\n",
        "#    - Classic dip-and-recovery pattern observed\n",
        "#    - Minimum ISQ at week X\n",
        "#    - Smooth trajectories suggest autocorrelation\n",
        "#\n",
        "# 3. BASELINE OLS MODEL\n",
        "#    - ISQ ~ week + week¬≤ + torque + cortical_thickness\n",
        "#    - R¬≤ = X.XX\n",
        "#    - Week coefficient: Œ≤‚ÇÅ = X.XX (negative = decline)\n",
        "#    - Week¬≤ coefficient: Œ≤‚ÇÇ = X.XX (positive = U-shape)\n",
        "#\n",
        "# 4. AUTOCORRELATION DIAGNOSTICS\n",
        "#    - Durbin-Watson: X.XX (< 2 ‚Üí positive autocorrelation)\n",
        "#    - ACF lag-1: X.XX (significant)\n",
        "#    - Ljung-Box p-value: < 0.001 (reject H‚ÇÄ: no autocorrelation)\n",
        "#    ‚Üí Conclusion: Strong positive autocorrelation detected\n",
        "#\n",
        "# 5. REMEDIES COMPARISON\n",
        "#    Approach         | DW    | SE Increase | Interpretation Change\n",
        "#    -----------------|-------|-------------|----------------------\n",
        "#    OLS baseline     | X.XX  | 0% (ref)    | Time trend\n",
        "#    Lagged ISQ       | X.XX  | N/A         | Autoregressive\n",
        "#    GLS AR(1)        | N/A   | +X%         | Time trend\n",
        "#    HAC (Newey-West) | X.XX  | +X%         | Time trend\n",
        "#\n",
        "# 6. RECOMMENDATION\n",
        "#    Best approach: [GLS AR(1) / HAC SEs]\n",
        "#    Reason: [Corrects inference while preserving interpretation]\n",
        "#\n",
        "# 7. CLINICAL IMPLICATIONS\n",
        "#    - ISQ measurements are NOT independent across time\n",
        "#    - Standard OLS underestimates uncertainty by X%\n",
        "#    - Recommended: use GLS or HAC SEs for valid hypothesis tests\n",
        "#    - Healing pattern: [describe dip timing and recovery]\n",
        "#\n",
        "# 8. KEY FINDINGS\n",
        "#    - Torque effect: Œ≤ = X.XX (95% CI: [X.XX, X.XX])\n",
        "#    - Dip timing: week X (lowest ISQ)\n",
        "#    - Recovery rate: Œ≤(week¬≤) = X.XX\n",
        "#\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Learning Reflection\n",
        "\n",
        "### Key Concepts Mastered\n",
        "‚úÖ Detecting autocorrelation (ACF, Durbin-Watson, Ljung-Box)  \n",
        "‚úÖ Understanding why autocorrelation matters (invalid SEs)  \n",
        "‚úÖ Applying remedies (lagged predictors, GLS, HAC)  \n",
        "‚úÖ Choosing the right approach for clinical data  \n",
        "\n",
        "### Next Steps\n",
        "1. Integrate findings with **01_multicollinearity_lab.ipynb** for comprehensive analysis\n",
        "2. Apply these techniques to your own longitudinal dental data\n",
        "3. Practice explaining autocorrelation to clinical collaborators\n",
        "\n",
        "### ü§î Reflection Questions\n",
        "- When would you use lagged variables vs. GLS in your research?\n",
        "- How would you explain Durbin-Watson to a clinician?\n",
        "- What other dental outcomes have autocorrelation issues? (Pain scores? Healing indices?)\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Further Reading\n",
        "\n",
        "### Textbooks\n",
        "- **Wooldridge (2015)**: *Introductory Econometrics* ‚Äî Chapters on serial correlation and dynamics\n",
        "- **Greene (2018)**: *Econometric Analysis* ‚Äî GLS and HAC standard errors\n",
        "- **Hamilton (1994)**: *Time Series Analysis* ‚Äî AR models and diagnostics\n",
        "\n",
        "### Papers\n",
        "- **Newey & West (1987)**: \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix\" ‚Äî Original HAC paper\n",
        "- **Cochrane & Orcutt (1949)**: \"Application of Least Squares Regression to Relationships Containing Auto-Correlated Error Terms\" ‚Äî Classic AR(1) correction\n",
        "\n",
        "### Clinical Applications\n",
        "- **Abrahamsson et al. (2004)**: ISQ monitoring during healing (see methods for time-series handling)\n",
        "- **Sennerby & Meredith (2008)**: \"Implant stability measurements using resonance frequency analysis\" ‚Äî Review of ISQ trajectories\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've completed the autocorrelation lab. ü¶∑üìä‚ú®\n",
        "\n",
        "You now understand one of the most common‚Äîand most overlooked‚Äîproblems in dental time-series research.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
